# FOV ë‚´ ë‹¤ë¥¸ ë¡œë´‡ Trajectoryë¥¼ í™œìš©í•œ ê°•í™”í•™ìŠµ êµ¬í˜„

ì´ ë¬¸ì„œëŠ” ë‹¤ì¤‘ ë¡œë´‡ íƒì‚¬ ì‹œë®¬ë ˆì´ì…˜ì—ì„œ FOV(Field of View) ë‚´ì— ê°ì§€ëœ ë‹¤ë¥¸ ë¡œë´‡ì˜ trajectoryë¥¼ ê°•í™”í•™ìŠµì— í™œìš©í•˜ê¸° ìœ„í•œ êµ¬í˜„ ë‚´ìš©ì„ ì •ë¦¬í•©ë‹ˆë‹¤.

## ğŸ“‹ êµ¬í˜„ ê°œìš”

**ëª©í‘œ**: ë¡œë´‡ì´ ìì‹ ì˜ FOV ë‚´ì— ë‹¤ë¥¸ ë¡œë´‡ì„ ê°ì§€í•˜ë©´, í•´ë‹¹ ë¡œë´‡ë“¤ì˜ ìµœê·¼ trajectoryë¥¼ ì¶”ì¶œí•˜ì—¬ ê°•í™”í•™ìŠµ ì‹œ observationì— í¬í•¨

**í•µì‹¬ ê¸°ìˆ **:
- **Temporal Transformer**: ì‹œê³„ì—´ trajectory ë°ì´í„°ë¥¼ ì²˜ë¦¬
- **Multi-Head Attention**: ì—¬ëŸ¬ ë¡œë´‡ì˜ trajectoryë¥¼ aggregation
- **Positional Encoding**: ì‹œê°„ ì •ë³´ ì¸ì½”ë”©

---

## ğŸ”§ ì£¼ìš” ë³€ê²½ ì‚¬í•­

### 1. **parameter.py** - Trajectory íŒŒë¼ë¯¸í„° ì¶”ê°€

```python
# Trajectory tracking parameters
TRAJECTORY_HISTORY_LENGTH = 10  # Number of recent steps to track
TRAJECTORY_FEATURE_DIM = 4      # (dx, dy, heading, velocity)
TRAJECTORY_EMBEDDING_DIM = 64   # Trajectory encoder output dimension
MAX_DETECTED_AGENTS = N_AGENTS - 1  # Maximum number of detectable agents in FOV
```

**ìœ„ì¹˜**: Line 80-84

---

### 2. **utils/multi_agent_worker.py** - Trajectory Buffer êµ¬í˜„

#### ë³€ê²½ ì‚¬í•­:
1. **Import ì¶”ê°€**:
   ```python
   from collections import deque
   ```

2. **Trajectory Buffer ì´ˆê¸°í™”** (`__init__` ë©”ì„œë“œ):
   ```python
   # Initialize trajectory buffer for each agent
   self.trajectory_buffer = {}
   for i in range(self.n_agents):
       self.trajectory_buffer[i] = deque(maxlen=TRAJECTORY_HISTORY_LENGTH)
       # Initialize with starting positions (x, y, heading, velocity=0)
       start_location = self.env.robot_locations[i]
       self.trajectory_buffer[i].append((
           start_location[0],
           start_location[1],
           self.env.angles[i],
           0.0
       ))
   ```

3. **Trajectory ì—…ë°ì´íŠ¸** (`run_episode` ë©”ì„œë“œ, reward ê³„ì‚° ë¶€ë¶„):
   ```python
   # Update trajectory buffer
   prev_trajectory = self.trajectory_buffer[robot.id][-1] if len(self.trajectory_buffer[robot.id]) > 0 else None
   if prev_trajectory is not None:
       prev_x, prev_y = prev_trajectory[0], prev_trajectory[1]
       velocity = np.linalg.norm(next_location - np.array([prev_x, prev_y])) / NUM_SIM_STEPS
   else:
       velocity = 0.0

   self.trajectory_buffer[robot.id].append((
       next_location[0],
       next_location[1],
       robot.heading,
       velocity
   ))
   ```

4. **Observation ìƒì„± ì‹œ trajectory_buffer ì „ë‹¬**:
   ```python
   # ê¸°ì¡´ ì½”ë“œ (2ê³³):
   observation = robot.get_observation()

   # ë³€ê²½ í›„:
   observation = robot.get_observation(
       robot_locations=self.env.robot_locations,
       trajectory_buffer=self.trajectory_buffer
   )
   ```

**ìœ„ì¹˜**: Line 27, 64-75, 168-181, 91-94, 241-244

---

### 3. **utils/agent.py** - FOV ê°ì§€ ë° Trajectory ì¶”ì¶œ

#### ì¶”ê°€ëœ ë©”ì„œë“œ:

1. **`get_robots_in_fov(self, robot_locations)`**:
   - FOV ë‚´ì— ìˆëŠ” ë‹¤ë¥¸ ë¡œë´‡ ê°ì§€
   - ê±°ë¦¬ ì²´í¬ (sensor_range ì´ë‚´)
   - ê°ë„ ì²´í¬ (FOV ë²”ìœ„ ë‚´)
   - ë°˜í™˜: ê°ì§€ëœ ë¡œë´‡ ID ë¦¬ìŠ¤íŠ¸

2. **`_get_detected_trajectories(self, robot_locations, trajectory_buffer)`**:
   - ê°ì§€ëœ ë¡œë´‡ë“¤ì˜ trajectory ì¶”ì¶œ
   - ìƒëŒ€ ì¢Œí‘œë¡œ ë³€í™˜ ë° ì •ê·œí™”
   - Padding ì²˜ë¦¬ (MAX_DETECTED_AGENTSê¹Œì§€)
   - ë°˜í™˜: (detected_trajectories, trajectory_mask) í…ì„œ

3. **`get_observation()` ë©”ì„œë“œ ìˆ˜ì •**:
   ```python
   # ê¸°ì¡´ signature:
   def get_observation(self, pad=True):

   # ë³€ê²½ í›„:
   def get_observation(self, pad=True, robot_locations=None, trajectory_buffer=None):

   # ë°˜í™˜ê°’ ë³€ê²½ (9ê°œ â†’ 11ê°œ):
   return [node_inputs, node_padding_mask, edge_mask, current_index, current_edge, edge_padding_mask,
           all_node_frontier_distribution, node_heading_visited, node_neighbor_best_headings,
           detected_trajectories, trajectory_mask]  # 2ê°œ ì¶”ê°€
   ```

4. **`select_next_waypoint()` ë©”ì„œë“œ ìˆ˜ì •**:
   ```python
   # ê¸°ì¡´:
   _, _, _, _, current_edge, _, _, _, _ = observation
   logp = self.policy_net(*observation)

   # ë³€ê²½ í›„:
   _, _, _, _, current_edge, _, _, _, _, _, _ = observation
   logp = self.policy_net(*observation[:9], detected_trajectories=observation[9], trajectory_mask=observation[10])
   ```

5. **`save_observation()` ë©”ì„œë“œ ìˆ˜ì •**:
   ```python
   # ê¸°ì¡´:
   node_inputs, ..., neighbor_best_headings = observation

   # ë³€ê²½ í›„:
   node_inputs, ..., neighbor_best_headings, detected_trajectories, trajectory_mask = observation
   # Note: detected_trajectoriesì™€ trajectory_maskëŠ” episode_bufferì— ì €ì¥í•˜ì§€ ì•ŠìŒ
   ```

**ìœ„ì¹˜**: Line 182, 245-255, 257-260, 384-486, 510-521, 544

---

### 4. **utils/model.py** - Trajectory Encoder ë° Network í†µí•©

#### ìƒˆë¡œ ì¶”ê°€ëœ í´ë˜ìŠ¤:

1. **`PositionalEncoding`**:
   - ì‹œê°„ ì •ë³´ë¥¼ ì¸ì½”ë”©í•˜ëŠ” Positional Encoding
   - Sinusoidal ë°©ì‹ ì‚¬ìš©

2. **`TrajectoryEncoder`**:
   ```python
   class TrajectoryEncoder(nn.Module):
       def __init__(self, feature_dim, trajectory_embedding_dim, seq_len, n_head=4, n_layer=2):
           # Feature projection
           self.feature_projection = nn.Linear(feature_dim, trajectory_embedding_dim)

           # Positional encoding
           self.positional_encoding = PositionalEncoding(trajectory_embedding_dim, max_len=seq_len)

           # Temporal transformer encoder
           self.temporal_encoder = Encoder(embedding_dim=trajectory_embedding_dim, n_head=n_head, n_layer=n_layer)

           # Agent aggregation
           self.agent_attention = MultiHeadAttention(trajectory_embedding_dim, n_heads=n_head)

           # Output projection
           self.output_layer = nn.Sequential(...)
   ```

   **ì²˜ë¦¬ íë¦„**:
   1. Input: `[batch, max_detected_agents, seq_len, feature_dim]`
   2. Feature Projection â†’ Positional Encoding
   3. Temporal Transformer (ê° agentì˜ trajectory ë…ë¦½ì ìœ¼ë¡œ ì¸ì½”ë”©)
   4. Agent Aggregation (Multi-Head Attention)
   5. Output: `[batch, trajectory_embedding_dim]`

#### PolicyNet ìˆ˜ì •:

```python
class PolicyNet(nn.Module):
    def __init__(self, node_dim, embedding_dim, num_angles_bin, use_trajectory=True):
        # Trajectory encoder ì¶”ê°€
        if use_trajectory:
            self.trajectory_encoder = TrajectoryEncoder(...)
            self.trajectory_fusion = nn.Linear(embedding_dim + TRAJECTORY_EMBEDDING_DIM, embedding_dim)

    def decode_state(self, ..., trajectory_embedding=None):
        # Trajectory embeddingê³¼ current stateë¥¼ fusion
        if self.use_trajectory and trajectory_embedding is not None:
            trajectory_embedding_expanded = trajectory_embedding.unsqueeze(1)
            fused = torch.cat([enhanced_current_node_feature, trajectory_embedding_expanded], dim=-1)
            enhanced_current_node_feature = self.trajectory_fusion(fused)

    def forward(self, ..., detected_trajectories=None, trajectory_mask=None):
        # Trajectory encoding
        if self.use_trajectory and detected_trajectories is not None:
            trajectory_embedding = self.trajectory_encoder(detected_trajectories, trajectory_mask)

        # Decode with trajectory fusion
        current_node_feature, enhanced_current_node_feature = self.decode_state(
            ..., trajectory_embedding)
```

#### QNet ìˆ˜ì •:

PolicyNetê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •:
- Trajectory encoder ì¶”ê°€
- `decode_state()` ë©”ì„œë“œì— trajectory fusion ì¶”ê°€
- `forward()` ë©”ì„œë“œì— trajectory encoding ì¶”ê°€

**ìœ„ì¹˜**: Line 1-4 (import), 199-308 (TrajectoryEncoder), 312-427 (PolicyNet), 430-568 (QNet)

---

## ğŸ¯ ì£¼ìš” íŠ¹ì§•

### 1. **Temporal Transformer Architecture**
- **ì‹œê°„ ì˜ì¡´ì„± í•™ìŠµ**: Positional Encoding + Self-Attention
- **ë³‘ë ¬ ì²˜ë¦¬**: ëª¨ë“  agentì˜ trajectoryë¥¼ ë™ì‹œì— ì²˜ë¦¬
- **ìœ ì—°í•œ ì‹œí€€ìŠ¤ ê¸¸ì´**: Paddingìœ¼ë¡œ ë‹¤ì–‘í•œ ê¸¸ì´ ì§€ì›

### 2. **Multi-Agent Trajectory Aggregation**
- **Cross-Attention**: ì—¬ëŸ¬ ë¡œë´‡ì˜ trajectoryë¥¼ í†µí•©
- **Adaptive Weighting**: Attention mechanismìœ¼ë¡œ ì¤‘ìš”ë„ ìë™ í•™ìŠµ
- **Scalable**: ê°ì§€ëœ ë¡œë´‡ ìˆ˜ì— ê´€ê³„ì—†ì´ ê³ ì •ëœ ì¶œë ¥ ì°¨ì›

### 3. **Feature Normalization**
- **ìƒëŒ€ ì¢Œí‘œ**: í˜„ì¬ ë¡œë´‡ ìœ„ì¹˜ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™”
- **ë²”ìœ„ ì •ê·œí™”**: ëª¨ë“  featureë¥¼ [-1, 1] ë˜ëŠ” [0, 1] ë²”ìœ„ë¡œ ë³€í™˜
- **ì•ˆì •ì ì¸ í•™ìŠµ**: Gradient ì•ˆì •ì„± í–¥ìƒ

---

## ğŸ“Š Trajectory Feature êµ¬ì„±

ê° trajectory stepì€ 4ì°¨ì› featureë¡œ í‘œí˜„ë©ë‹ˆë‹¤:

```python
feature = [dx_norm, dy_norm, heading_norm, velocity_norm]
```

1. **dx_norm, dy_norm**: ìƒëŒ€ ì¢Œí‘œ (í˜„ì¬ ë¡œë´‡ ê¸°ì¤€)
   - ì •ê·œí™”: `/ (UPDATING_MAP_SIZE / 2)`

2. **heading_norm**: ë°©í–¥ (0-360ë„)
   - ì •ê·œí™”: `/ 360.0` â†’ [0, 1]

3. **velocity_norm**: ì†ë„
   - ì •ê·œí™”: `/ (VELOCITY * NUM_SIM_STEPS)`

---

## ğŸš€ ì‚¬ìš© ë°©ë²•

### í•™ìŠµ ì‹œì‘:
```bash
conda activate marvel
python driver.py
```

### Trajectory ê¸°ëŠ¥ ë„ê¸°:
```python
# utils/model.pyì—ì„œ
policy_net = PolicyNet(NODE_INPUT_DIM, EMBEDDING_DIM, NUM_ANGLES_BIN, use_trajectory=False)
q_net = QNet(NODE_INPUT_DIM, EMBEDDING_DIM, NUM_ANGLES_BIN, TRAIN_ALGO, use_trajectory=False)
```

---

## ğŸ” ë””ë²„ê¹… ë° í…ŒìŠ¤íŠ¸

### 1. Network ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸:
```python
from parameter import *
from utils.model import PolicyNet, QNet
import torch

policy_net = PolicyNet(NODE_INPUT_DIM, EMBEDDING_DIM, NUM_ANGLES_BIN, use_trajectory=True)
q_net = QNet(NODE_INPUT_DIM, EMBEDDING_DIM, NUM_ANGLES_BIN, TRAIN_ALGO, use_trajectory=True)
print("Networks initialized successfully!")
```

### 2. FOV ê°ì§€ í…ŒìŠ¤íŠ¸:
```python
from utils.agent import Agent

# agent.get_robots_in_fov(robot_locations) í˜¸ì¶œ
detected_ids = agent.get_robots_in_fov(robot_locations)
print(f"Detected robots: {detected_ids}")
```

### 3. Trajectory Buffer í™•ì¸:
```python
from utils.multi_agent_worker import MultiAgentWorker

# worker.trajectory_buffer ì¶œë ¥
for agent_id, trajectory in worker.trajectory_buffer.items():
    print(f"Agent {agent_id}: {len(trajectory)} steps")
```

---

## ğŸ“ êµ¬í˜„ ì‹œ ì£¼ì˜ì‚¬í•­

### 1. **Episode Buffer**
- Trajectory ì •ë³´ëŠ” **ì‹¤ì‹œê°„ìœ¼ë¡œë§Œ** ì‚¬ìš©ë©ë‹ˆë‹¤
- Episode bufferì—ëŠ” ì €ì¥ë˜ì§€ ì•ŠìŒ (ë§¤ë²ˆ ìƒˆë¡œ ê³„ì‚°)
- ì´ìœ : TrajectoryëŠ” í™˜ê²½ ìƒíƒœì— ì˜ì¡´í•˜ë¯€ë¡œ ì €ì¥ ì‹œ ë©”ëª¨ë¦¬ ë¶€ë‹´ ë° ì¬í˜„ì„± ë¬¸ì œ

### 2. **Observation êµ¬ì¡° ë³€ê²½**
- ê¸°ì¡´ 9ê°œ ìš”ì†Œ â†’ 11ê°œ ìš”ì†Œë¡œ í™•ì¥
- ê¸°ì¡´ ì½”ë“œì—ì„œ observation unpacking ì‹œ ì£¼ì˜ í•„ìš”

### 3. **Backward Compatibility**
- `use_trajectory=False`ë¡œ ì„¤ì • ì‹œ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ë™ì‘
- ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹œ í˜¸í™˜ì„± í™•ì¸ í•„ìš”

### 4. **Performance ê³ ë ¤ì‚¬í•­**
- Trajectory EncoderëŠ” ì¶”ê°€ ì—°ì‚° ë¹„ìš© ë°œìƒ
- GPU ì‚¬ìš© ê¶Œì¥ (`USE_GPU_GLOBAL = True`)

---

## ğŸ¨ Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Multi-Robot Environment                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚ Robot 0 â”‚  â”‚ Robot 1 â”‚  â”‚ Robot 2 â”‚  â”‚ Robot 3 â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚           â”‚            â”‚             â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
              â”‚ FOV Detectionâ”‚
              â”‚ (Agent.get_  â”‚
              â”‚ robots_in_   â”‚
              â”‚ fov())       â”‚
              â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Trajectory Extraction   â”‚
        â”‚ (Recent N steps)        â”‚
        â”‚ - Position (x, y)       â”‚
        â”‚ - Heading (Î¸)           â”‚
        â”‚ - Velocity (v)          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Feature Projection    â”‚
        â”‚   + Positional Encoding â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Temporal Transformer    â”‚
        â”‚ (Self-Attention over    â”‚
        â”‚  time steps)            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Agent Aggregation      â”‚
        â”‚  (Cross-Attention)      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Trajectory Embedding   â”‚
        â”‚  [batch, 64]            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   State Fusion          â”‚
        â”‚   (Concat + Linear)     â”‚
        â”‚                         â”‚
        â”‚   Current State â”€â”€â”€â”€â”   â”‚
        â”‚                     â”‚   â”‚
        â”‚   Trajectory â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â–ºâ”‚
        â”‚   Embedding            â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Policy / Q-Network    â”‚
        â”‚   Action Selection      â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§ª í…ŒìŠ¤íŠ¸ ì½”ë“œ ìˆ˜ì • ì‚¬í•­

test_driver.pyë¥¼ í†µí•´ í•™ìŠµëœ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•  ë•Œë„ trajectory encoderê°€ ë°˜ì˜ë˜ë„ë¡ ë‹¤ìŒ íŒŒì¼ë“¤ì„ ìˆ˜ì •í–ˆìŠµë‹ˆë‹¤:

### 1. **test_parameter.py** ìˆ˜ì •

```python
# Network parameters
USE_TRAJECTORY = True  # Enable trajectory encoder

# Trajectory tracking parameters (same as parameter.py)
TRAJECTORY_HISTORY_LENGTH = 10
TRAJECTORY_FEATURE_DIM = 4
TRAJECTORY_EMBEDDING_DIM = 64
MAX_DETECTED_AGENTS = 10  # Conservative estimate for test
```

**ìœ„ì¹˜**: Line 59-66

### 2. **test_driver.py** ìˆ˜ì •

#### Runner í´ë˜ìŠ¤ì˜ network ì´ˆê¸°í™”:
```python
# ê¸°ì¡´:
self.local_network = PolicyNet(NODE_INPUT_DIM, EMBEDDING_DIM, NUM_ANGLES_BIN)

# ë³€ê²½ í›„:
self.local_network = PolicyNet(NODE_INPUT_DIM, EMBEDDING_DIM, NUM_ANGLES_BIN, use_trajectory=USE_TRAJECTORY)
```

**ìœ„ì¹˜**: Line 128

### 3. **utils/test_worker.py** ìˆ˜ì •

#### Import ì¶”ê°€:
```python
from collections import deque
```

#### Trajectory Buffer ì´ˆê¸°í™” (`__init__` ë©”ì„œë“œ):
```python
# Initialize trajectory buffer for each agent
self.trajectory_buffer = {}
for i in range(self.n_agents):
    self.trajectory_buffer[i] = deque(maxlen=TRAJECTORY_HISTORY_LENGTH)
    start_location = self.env.robot_locations[i]
    self.trajectory_buffer[i].append((
        start_location[0],
        start_location[1],
        self.env.angles[i],
        0.0
    ))
```

**ìœ„ì¹˜**: Line 6, 42-53

#### Observation ìƒì„± ì‹œ trajectory_buffer ì „ë‹¬:
```python
# ê¸°ì¡´:
observation = robot.get_observation(pad=False)

# ë³€ê²½ í›„:
observation = robot.get_observation(
    pad=False,
    robot_locations=self.env.robot_locations,
    trajectory_buffer=self.trajectory_buffer
)
```

**ìœ„ì¹˜**: Line 83-87

#### Trajectory ì—…ë°ì´íŠ¸:
```python
# Update trajectory buffer
prev_trajectory = self.trajectory_buffer[robot.id][-1] if len(self.trajectory_buffer[robot.id]) > 0 else None
if prev_trajectory is not None:
    prev_x, prev_y = prev_trajectory[0], prev_trajectory[1]
    velocity = np.linalg.norm(next_location - np.array([prev_x, prev_y])) / NUM_SIM_STEPS
else:
    velocity = 0.0

self.trajectory_buffer[robot.id].append((
    next_location[0],
    next_location[1],
    robot.heading,
    velocity
))
```

**ìœ„ì¹˜**: Line 159-172

#### `__main__` ë¶€ë¶„ ìˆ˜ì •:
```python
# ê¸°ì¡´:
policy_net = PolicyNet(NODE_INPUT_DIM, EMBEDDING_DIM, NUM_ANGLES_BIN)

# ë³€ê²½ í›„:
policy_net = PolicyNet(NODE_INPUT_DIM, EMBEDDING_DIM, NUM_ANGLES_BIN, use_trajectory=USE_TRAJECTORY)
```

**ìœ„ì¹˜**: Line 377

### í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë°©ë²•:

```bash
conda activate marvel
python test_driver.py
```

### ì£¼ì˜ì‚¬í•­:

1. **ì²´í¬í¬ì¸íŠ¸ í˜¸í™˜ì„±**:
   - Trajectory encoderê°€ í¬í•¨ëœ ëª¨ë¸ë¡œ í•™ìŠµëœ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤
   - ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ `USE_TRAJECTORY = False`ë¡œ ì„¤ì •í•˜ì„¸ìš”

2. **ë™ì  Agent ìˆ˜**:
   - Testì—ì„œëŠ” agent ìˆ˜ê°€ ë™ì ìœ¼ë¡œ ë³€ê²½ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤
   - `MAX_DETECTED_AGENTS`ë¥¼ ì¶©ë¶„íˆ í¬ê²Œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤ (10)

3. **ì„±ëŠ¥ ë¹„êµ**:
   - Trajectory ê¸°ëŠ¥ ON/OFFë¡œ ì„±ëŠ¥ ë¹„êµ ê°€ëŠ¥
   - `test_parameter.py`ì˜ `USE_TRAJECTORY` í”Œë˜ê·¸ë¡œ ì œì–´

---

## ğŸ¨ í–¥ìƒëœ ì‹œê°í™” ê¸°ëŠ¥

test_driver.pyë¥¼ í†µí•´ ìƒì„±ë˜ëŠ” GIFì— FOV ë‚´ ê°ì§€ëœ ë¡œë´‡ì˜ trajectoryì™€ ê° agentì˜ local viewë¥¼ ì‹œê°ì ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.

### ì „ì²´ ë ˆì´ì•„ì›ƒ (2í–‰ êµ¬ì¡°):

#### **ìƒë‹¨ í–‰ (Global View)**:
1. **ì™¼ìª½ íŒ¨ë„ - Global Belief Map**:
   - ëª¨ë“  ë¡œë´‡ì˜ trajectoryë¥¼ ë°˜íˆ¬ëª…í•˜ê²Œ í‘œì‹œ (alpha=0.4)
   - ê°ì§€ëœ ë¡œë´‡ì˜ trajectoryë¥¼ ë‘ê»ê³  ì ì„ ìœ¼ë¡œ ê°•ì¡° (linewidth=3.0, linestyle='--')
   - í˜„ì¬ ìœ„ì¹˜ì— í°ìƒ‰ í…Œë‘ë¦¬ì˜ ì›í˜• ë§ˆì»¤ ì¶”ê°€
   - Global frontiers í‘œì‹œ

2. **ì˜¤ë¥¸ìª½ íŒ¨ë„ - FOV & Detections**:
   - FOV Cone: ê° ë¡œë´‡ì˜ ì‹œì•¼ ë²”ìœ„ë¥¼ ë¶€ì±„ê¼´ë¡œ í‘œì‹œ
   - Detection Links: ê°ì§€í•˜ëŠ” ë¡œë´‡ê³¼ ê°ì§€ëœ ë¡œë´‡ ì‚¬ì´ë¥¼ í°ìƒ‰ ì ì„ ìœ¼ë¡œ ì—°ê²°
   - ê°ì§€ëœ Trajectory ê°•ì¡°
   - Detection Summary: ì œëª©ì— ê° ë¡œë´‡ì´ ê°ì§€í•œ ë‹¤ë¥¸ ë¡œë´‡ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ í‘œì‹œ

#### **í•˜ë‹¨ í–‰ (Individual Agent Local Views)** - ğŸ†• ìƒˆë¡œ ì¶”ê°€:
ê° agentë§ˆë‹¤ ê°œë³„ íŒ¨ë„ë¡œ ìì‹ ì˜ local observationì„ í‘œì‹œ:

1. **Local Map**:
   - ê° agentì˜ í˜„ì¬ ìœ„ì¹˜ ì¤‘ì‹¬ìœ¼ë¡œ UPDATING_MAP_SIZE ë²”ìœ„ ë‚´ ì§€ë„ í‘œì‹œ
   - Agentê°€ ì‹¤ì œë¡œ decision-makingì— ì‚¬ìš©í•˜ëŠ” local view ì‹œê°í™”

2. **FOV Cone**:
   - Agentì˜ ì‹œì•¼ ë²”ìœ„ë¥¼ ë°˜íˆ¬ëª… ë¶€ì±„ê¼´ë¡œ í‘œì‹œ
   - Agentê°€ í˜„ì¬ ì–´ëŠ ë°©í–¥ì„ ë³´ê³  ìˆëŠ”ì§€ ëª…í™•íˆ í‘œì‹œ

3. **Detected Robots (FOV ë‚´ ê°ì§€ëœ ë‹¤ë¥¸ ë¡œë´‡)**:
   - **ê°ì§€ëœ ë¡œë´‡**: í° ì›í˜• ë§ˆì»¤ + ë…¸ë€ìƒ‰ í…Œë‘ë¦¬ (markeredgecolor='yellow')
   - **Detection Line**: Agentì™€ ê°ì§€ëœ ë¡œë´‡ ì‚¬ì´ ë…¸ë€ìƒ‰ ì ì„ ìœ¼ë¡œ ì—°ê²°
   - **ë¹„ê°ì§€ ë¡œë´‡**: ì‘ê³  ë°˜íˆ¬ëª…í•œ ë§ˆì»¤ (alpha=0.5)

4. **Local Frontiers**:
   - Agentê°€ ê´€ì¸¡í•˜ëŠ” frontiersë¥¼ ë¹¨ê°„ ì ìœ¼ë¡œ í‘œì‹œ
   - Agentì˜ exploration ëª©í‘œ ì§€ì  ì‹œê°í™”

5. **Title**:
   - Agent ì´ë¦„ (ìƒ‰ìƒê³¼ í•¨ê»˜)
   - í˜„ì¬ ê°ì§€ ì¤‘ì¸ ë‹¤ë¥¸ robot ëª©ë¡ í‘œì‹œ

### ì‹œê°í™” ì½”ë“œ êµ¬í˜„:

**ìœ„ì¹˜**: utils/test_worker.py, Line 315-589

#### ì£¼ìš” ê¸°ëŠ¥:

1. **ë ˆì´ì•„ì›ƒ ìë™ ì¡°ì •** (Line 318-320):
   ```python
   n_cols = max(2, self.n_agents)
   fig = plt.figure(figsize=(3 * n_cols, 6))
   ```
   - Agent ìˆ˜ì— ë”°ë¼ ì—´ ê°œìˆ˜ ìë™ ì¡°ì •
   - ìƒë‹¨ 2ê°œ íŒ¨ë„, í•˜ë‹¨ agent ìˆ˜ë§Œí¼ íŒ¨ë„ ìƒì„±

2. **Local Map ì¶”ì¶œ** (Line 477-486):
   ```python
   center_cell = robot_locations[robot.id]
   half_size = local_map_size // 2

   row_start = max(0, int(center_cell[1] - half_size))
   row_end = min(self.env.robot_belief.shape[0], int(center_cell[1] + half_size))
   col_start = max(0, int(center_cell[0] - half_size))
   col_end = min(self.env.robot_belief.shape[1], int(center_cell[0] + half_size))

   local_map = self.env.robot_belief[row_start:row_end, col_start:col_end]
   ```

3. **FOV ë‚´ ë‹¤ë¥¸ Robot ì‹œê°í™”** (Line 519-547):
   ```python
   # Check if this other robot is detected by current robot
   is_detected = other_robot.id in fov_detections.get(robot.id, [])

   if is_detected:
       # Highlight detected robots with yellow border
       plt.plot(other_local_x, other_local_y, 'o',
               color=other_c, markersize=10,
               markeredgewidth=3, markeredgecolor='yellow', zorder=15)
       # Draw detection line
       plt.plot([robot_local_x, other_local_x], [robot_local_y, other_local_y],
               'y--', linewidth=2, alpha=0.8, zorder=12)
   ```

### ì‹œê°í™” ì˜ˆì‹œ:

```
Title: Explored: 0.85  Distance: 45.2
       Headings: Red-90Â°, Blue-45Â°, Green-180Â°, Yellow-270Â°
       FOV Detections: Red detects: Blue, Green | Blue detects: Red

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  [Global Belief Map]       [FOV & Detections]              â”‚
â”‚  - All trajectories        - FOV cones                      â”‚
â”‚  - Detected highlighted    - Detection links                â”‚
â”‚  - Global frontiers        - Highlighted detections         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [Red Agent Local]  [Blue Agent Local]  [Green] [Yellow]   â”‚
â”‚  - Local map        - Local map         - ...   - ...      â”‚
â”‚  - FOV cone         - FOV cone                              â”‚
â”‚  - Detected: Blue   - Detected: Red                         â”‚
â”‚  - Local frontiers  - Local frontiers                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ì‹œê°ì  ìš”ì†Œ:

| ìš”ì†Œ | ìŠ¤íƒ€ì¼ | ì˜ë¯¸ |
|------|--------|------|
| **Global View** | | |
| ì¼ë°˜ Trajectory | ê°€ëŠ” ì‹¤ì„ , alpha=0.4 | ëª¨ë“  ë¡œë´‡ì˜ ì´ë™ ê²½ë¡œ |
| ê°ì§€ëœ Trajectory | ë‘êº¼ìš´ ì ì„ , alpha=1.0 | ë‹¤ë¥¸ ë¡œë´‡ì˜ FOVì— í¬ì°©ëœ ê²½ë¡œ |
| í˜„ì¬ ìœ„ì¹˜ ë§ˆì»¤ | í°ìƒ‰ í…Œë‘ë¦¬ ì› | ê°ì§€ëœ ë¡œë´‡ì˜ í˜„ì¬ ìœ„ì¹˜ |
| Detection Link | í°ìƒ‰ ì ì„  | ê°ì§€ ê´€ê³„ ì—°ê²°ì„  |
| FOV Cone | ë¶€ì±„ê¼´, alpha=0.3 | ë¡œë´‡ì˜ ì‹œì•¼ ë²”ìœ„ |
| **Local View (ê° Agent)** | | |
| Local Map | UPDATING_MAP_SIZE ë²”ìœ„ | Agentì˜ decision-making ì˜ì—­ |
| ê°ì§€ëœ ë¡œë´‡ | í° ì› + ë…¸ë€ìƒ‰ í…Œë‘ë¦¬ | FOV ë‚´ì—ì„œ ê°ì§€ëœ ë‹¤ë¥¸ ë¡œë´‡ |
| Detection Line | ë…¸ë€ìƒ‰ ì ì„  | Agentì™€ ê°ì§€ëœ ë¡œë´‡ ê°„ ì—°ê²° |
| ë¹„ê°ì§€ ë¡œë´‡ | ì‘ì€ ì›, alpha=0.5 | Local ì˜ì—­ ë‚´ ë¹„ê°ì§€ ë¡œë´‡ |
| Local Frontiers | ë¹¨ê°„ ì , s=2 | Agentê°€ ê´€ì¸¡í•˜ëŠ” frontier |

### ì‹¤ì œ í™œìš©:

ì´ ì‹œê°í™”ë¥¼ í†µí•´ ë‹¤ìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

1. **í†µì‹  ì—†ëŠ” í•™ìŠµ ê²€ì¦**:
   - ê° agentê°€ ë…ë¦½ì ì¸ local observationë§Œ ì‚¬ìš©í•˜ëŠ”ì§€ í™•ì¸
   - FOV ë°–ì˜ ë¡œë´‡ì€ ê°ì§€ë˜ì§€ ì•ŠìŒì„ ì‹œê°ì ìœ¼ë¡œ í™•ì¸

2. **Trajectory Encoder íš¨ê³¼**:
   - ê° agentê°€ ì–´ë–¤ ë‹¤ë¥¸ ë¡œë´‡ì„ ê°ì§€í•˜ê³  ìˆëŠ”ì§€ ëª…í™•íˆ í‘œì‹œ
   - Detection lineìœ¼ë¡œ information flow ì‹œê°í™”

3. **Decision-making ë¶„ì„**:
   - ê° agentì˜ local frontiersì™€ ì„ íƒí•œ ê²½ë¡œ ê´€ì°°
   - Agentê°€ ê°ì§€ëœ ë¡œë´‡ì„ í”¼í•˜ê±°ë‚˜ í˜‘ë ¥í•˜ëŠ” í–‰ë™ ë¶„ì„

4. **ì„±ëŠ¥ ë””ë²„ê¹…**:
   - Agentê°€ frontierë¥¼ ì œëŒ€ë¡œ ê°ì§€í•˜ëŠ”ì§€ í™•ì¸
   - FOV ë²”ìœ„ê°€ ì˜¬ë°”ë¥´ê²Œ ì ìš©ë˜ëŠ”ì§€ ê²€ì¦

---

## ğŸ“š ì°¸ê³  ìë£Œ

### Training ê´€ë ¨ íŒŒì¼:
- `parameter.py`: Line 80-84 (Trajectory parameters)
- `driver.py`: PolicyNet/QNet ì´ˆê¸°í™” ë¶€ë¶„
- `utils/multi_agent_worker.py`: Line 27, 64-75, 168-181, 91-94, 241-244
- `utils/agent.py`: Line 182, 245-255, 257-260, 384-486, 510-521, 544
- `utils/model.py`: Line 1-4, 199-308, 312-427, 430-568

### Testing ê´€ë ¨ íŒŒì¼:
- `test_parameter.py`:
  - Line 59-66: Trajectory parameters
  - Line 77-79: Communication settings
- `test_driver.py`: Line 40-45 (global_network), Line 128 (Runner.local_network)
- `utils/test_worker.py`:
  - Line 6, 42-53: Trajectory buffer ì´ˆê¸°í™”
  - Line 83-87, 159-172: Trajectory buffer ì—…ë°ì´íŠ¸ ë° ì‚¬ìš©
  - Line 284-313: `get_detected_robots_in_fov()` - FOV ë‚´ ë¡œë´‡ ê°ì§€ í•¨ìˆ˜
  - Line 315-589: `plot_local_env_sim()` - í–¥ìƒëœ ì‹œê°í™”
    - Line 318-320: 2í–‰ ë ˆì´ì•„ì›ƒ êµ¬ì¡° (ìƒë‹¨: global view, í•˜ë‹¨: per-agent local views)
    - Line 331-392: Global belief map íŒ¨ë„
    - Line 394-465: FOV & detections íŒ¨ë„
    - Line 467-567: **ê° Agentë³„ local view íŒ¨ë„ (ìƒˆë¡œ ì¶”ê°€)**

### í•µì‹¬ í•¨ìˆ˜:
- `MultiAgentWorker.__init__()`: Trajectory buffer ì´ˆê¸°í™”
- `Agent.get_robots_in_fov()`: FOV ë‚´ ë¡œë´‡ ê°ì§€
- `Agent._get_detected_trajectories()`: Trajectory ì¶”ì¶œ ë° ì¸ì½”ë”©
- `TrajectoryEncoder.forward()`: Temporal transformer ì²˜ë¦¬
- `PolicyNet.decode_state()`: Trajectory fusion

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

êµ¬í˜„ ì™„ë£Œ í•­ëª©:

### Trajectory Encoder êµ¬í˜„:
- [x] Trajectory íŒŒë¼ë¯¸í„° ì¶”ê°€ (parameter.py)
- [x] Trajectory buffer êµ¬í˜„ (multi_agent_worker.py)
- [x] FOV ê°ì§€ í•¨ìˆ˜ (agent.py)
- [x] Trajectory Encoder with Transformer (model.py)
- [x] PolicyNet í†µí•© (model.py)
- [x] QNet í†µí•© (model.py)
- [x] Observation ìƒì„± ì—…ë°ì´íŠ¸ (agent.py)

### í†µì‹  ì„¤ì • êµ¬í˜„:
- [x] USE_COMMUNICATION íŒŒë¼ë¯¸í„° ì¶”ê°€ (parameter.py, test_parameter.py)
- [x] effective_train_algo ë¡œì§ êµ¬í˜„ (driver.py)
- [x] ì¡°ê±´ë¶€ agent indices ì €ì¥ (multi_agent_worker.py)
- [x] í•™ìŠµ ë£¨í”„ state êµ¬ì„± ìˆ˜ì • (driver.py)
- [x] ë¬¸ì„œí™” (temp_readme.md)

### í–¥ìƒëœ ì‹œê°í™” êµ¬í˜„:
- [x] FOV ë‚´ ê°ì§€ëœ trajectory ê°•ì¡° (test_worker.py)
- [x] Detection links í‘œì‹œ (test_worker.py)
- [x] **ê° Agentë³„ local view ì¶”ê°€ (test_worker.py)** ğŸ†•
- [x] Local map ì¶”ì¶œ ë° í‘œì‹œ
- [x] FOV cone ì‹œê°í™”
- [x] Detected robots ê°•ì¡° í‘œì‹œ
- [x] Local frontiers í‘œì‹œ

### í…ŒìŠ¤íŠ¸ ë° í‰ê°€:
- [ ] ì‹¤ì œ í•™ìŠµ í…ŒìŠ¤íŠ¸
- [ ] í†µì‹  ìˆìŒ/ì—†ìŒ ì„±ëŠ¥ ë¹„êµ
- [ ] Local view ì‹œê°í™” ê²€ì¦

---

## ğŸ”Œ í†µì‹  ì„¤ì • (Communication Settings)

MARVELì€ ì´ì œ ì—ì´ì „íŠ¸ ê°„ í†µì‹  ì—¬ë¶€ë¥¼ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì™„ì „í•œ ì •ë³´ ê³µìœ (centralized) vs ì‹œê°ì  ê°ì§€ë§Œ ì‚¬ìš©(decentralized) ë‘ ê°€ì§€ í•™ìŠµ ëª¨ë“œë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

### USE_COMMUNICATION íŒŒë¼ë¯¸í„°

**parameter.py** (Line 100-104):
```python
USE_COMMUNICATION = False  # True: MAAC with all agent communication (centralized critic)
                           # False: Decentralized learning with only FOV-based trajectory observation
                           # When False, agents only use their own observation + detected trajectories in FOV
                           # This simulates no-communication scenario where agents rely on visual detection only
```

### í†µì‹  ëª¨ë“œë³„ ì°¨ì´ì 

#### 1. **USE_COMMUNICATION = True** (í†µì‹  ìˆìŒ)
- **í•™ìŠµ ì•Œê³ ë¦¬ì¦˜**: TRAIN_ALGOì— ë”°ë¼ MAAC ë˜ëŠ” MAAC+GT ì‚¬ìš©
- **ì •ë³´ ê³µìœ **: ëª¨ë“  ì—ì´ì „íŠ¸ì˜ ìœ„ì¹˜ì™€ ìƒíƒœ ì •ë³´ë¥¼ QNetì— ì „ë‹¬
- **Centralized Critic**: ê¸€ë¡œë²Œ ì •ë³´ë¥¼ í™œìš©í•œ ê°€ì¹˜ í‰ê°€
- **ì¥ì **: ë” ë§ì€ ì •ë³´ë¡œ í•™ìŠµ, ìˆ˜ë ´ ì†ë„ ë¹ ë¦„
- **ë‹¨ì **: ì‹¤ì œ í™˜ê²½ì—ì„œ í†µì‹  ì¸í”„ë¼ í•„ìš”

#### 2. **USE_COMMUNICATION = False** (í†µì‹  ì—†ìŒ)
- **í•™ìŠµ ì•Œê³ ë¦¬ì¦˜**: TRAIN_ALGOì—ì„œ í†µì‹  ìš”ì†Œ ì œê±°
  - TRAIN_ALGO 3 (MAAC+GT) â†’ effective_train_algo 2 (GT only)
  - TRAIN_ALGO 1 (MAAC) â†’ effective_train_algo 0 (SAC)
- **ì •ë³´ ê³µìœ **: ì—†ìŒ (ê° ì—ì´ì „íŠ¸ê°€ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµ)
- **ì‹œê°ì  ê°ì§€**: FOV ë‚´ ê°ì§€ëœ ë¡œë´‡ì˜ trajectoryë§Œ ì‚¬ìš©
- **ì¥ì **: í†µì‹  ì¸í”„ë¼ ë¶ˆí•„ìš”, ë” í˜„ì‹¤ì ì¸ ì‹œë‚˜ë¦¬ì˜¤
- **ë‹¨ì **: ì œí•œëœ ì •ë³´ë¡œ í•™ìŠµ, ìˆ˜ë ´ ì†ë„ ëŠë¦´ ìˆ˜ ìˆìŒ

### êµ¬í˜„ ìƒì„¸

#### 1. **driver.py** - Effective Training Algorithm ê³„ì‚°

**ìœ„ì¹˜**: Line 53-72

```python
# Determine effective training algorithm based on communication setting
# When USE_COMMUNICATION=False, disable agent communication in QNet
if USE_COMMUNICATION:
    effective_train_algo = TRAIN_ALGO
else:
    # Remove agent communication component from TRAIN_ALGO
    # TRAIN_ALGO 3 (MAAC + GT) -> 2 (GT only)
    # TRAIN_ALGO 1 (MAAC) -> 0 (SAC)
    if TRAIN_ALGO == 3:
        effective_train_algo = 2  # Ground Truth only, no communication
    elif TRAIN_ALGO == 1:
        effective_train_algo = 0  # SAC, no communication
    else:
        effective_train_algo = TRAIN_ALGO  # 0 or 2 already have no communication

print(f"Training Configuration:")
print(f"  TRAIN_ALGO: {TRAIN_ALGO}")
print(f"  USE_COMMUNICATION: {USE_COMMUNICATION}")
print(f"  Effective TRAIN_ALGO for QNet: {effective_train_algo}")
print(f"  Using Trajectory Encoder: True")
```

**Network ì´ˆê¸°í™”** (Line 75-82):
```python
global_policy_net = PolicyNet(NODE_INPUT_DIM, EMBEDDING_DIM, NUM_ANGLES_BIN, use_trajectory=True).to(device)
global_q_net1 = QNet(NODE_INPUT_DIM, EMBEDDING_DIM, NUM_ANGLES_BIN, effective_train_algo, use_trajectory=True).to(device)
global_q_net2 = QNet(NODE_INPUT_DIM, EMBEDDING_DIM, NUM_ANGLES_BIN, effective_train_algo, use_trajectory=True).to(device)
# ...
global_target_q_net1 = QNet(NODE_INPUT_DIM, EMBEDDING_DIM, NUM_ANGLES_BIN, effective_train_algo, use_trajectory=True).to(device)
global_target_q_net2 = QNet(NODE_INPUT_DIM, EMBEDDING_DIM, NUM_AGES_BIN, effective_train_algo, use_trajectory=True).to(device)
```

#### 2. **multi_agent_worker.py** - ì¡°ê±´ë¶€ Agent Indices ì €ì¥

**ìœ„ì¹˜**: Line 224-232

```python
curr_node_indices = np.array([robot.current_index for robot in self.robot_list])
for robot, reward in zip(self.robot_list, reward_list):
    robot.save_reward(reward + team_reward)
    # Only save all agent indices when communication is enabled
    # When USE_COMMUNICATION=False, agents rely solely on FOV-detected trajectories
    if USE_COMMUNICATION:
        robot.save_all_indices(curr_node_indices)
    robot.update_planning_state(self.env.robot_locations)
    robot.save_done(done)
```

**í•µì‹¬**: `USE_COMMUNICATION=False`ì¼ ë•ŒëŠ” `save_all_indices()`ë¥¼ í˜¸ì¶œí•˜ì§€ ì•Šì•„, episode bufferì— ë‹¤ë¥¸ ì—ì´ì „íŠ¸ì˜ ìœ„ì¹˜ ì •ë³´ê°€ ì €ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.

#### 2-1. **agent.py** - save_next_observations ìˆ˜ì •

**ìœ„ì¹˜**: Line 532-563

```python
def save_next_observations(self, observation, next_node_index_list):
    # ... (ê¸°ì¡´ ì½”ë“œ)

    # Only process agent indices if they were saved (USE_COMMUNICATION=True)
    if len(self.episode_buffer[35]) > 0:
        self.episode_buffer[36] = copy.deepcopy(self.episode_buffer[35])[1:]

    # ... (observation ì €ì¥)

    # Only update agent indices buffers if they were initialized
    if len(self.episode_buffer[35]) > 0:
        self.episode_buffer[36] += torch.tensor(next_node_index_list).reshape(1, -1, 1).to(self.device)
        self.episode_buffer[37] = copy.deepcopy(self.episode_buffer[36])[1:]
        self.episode_buffer[37] += copy.deepcopy(self.episode_buffer[36])[-1:]
```

**í•µì‹¬**: episode_buffer[35]ê°€ ë¹„ì–´ìˆì„ ë•Œ (USE_COMMUNICATION=False) episode_buffer[36], [37]ì„ ì²˜ë¦¬í•˜ì§€ ì•Šì•„ IndexErrorë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.

#### 3. **driver.py** - ë¹ˆ ë²„í¼ ì²˜ë¦¬

**ìœ„ì¹˜**: Line 200-212

```python
indices = range(len(experience_buffer[0]))

# training for n times each step
for j in range(4):
    # randomly sample a batch data
    sample_indices = random.sample(indices, BATCH_SIZE)
    rollouts = []
    for i in range(len(experience_buffer)):
        # Skip empty buffers (e.g., agent indices when USE_COMMUNICATION=False)
        if len(experience_buffer[i]) == 0:
            rollouts.append([])
        else:
            rollouts.append([experience_buffer[i][index] for index in sample_indices])
```

**í•µì‹¬**: experience_bufferì˜ ì¼ë¶€(35, 36, 37)ê°€ ë¹„ì–´ìˆì„ ë•Œ IndexErrorë¥¼ ë°©ì§€í•©ë‹ˆë‹¤.

#### 4. **driver.py** - í•™ìŠµ ë£¨í”„ì—ì„œ State êµ¬ì„±

**ìœ„ì¹˜**: Line 233-285

**Ground Truth ë°ì´í„° ë¡œë”©** (Line 233-250):
```python
# Load ground truth data if needed
if effective_train_algo in (2,3):
    gt_node_inputs = torch.stack(rollouts[19]).to(device)
    # ... (ground truth data loading)
```

**Agent Indices ë¡œë”©** (Line 252-257):
```python
# Load agent indices only when communication is enabled
# When USE_COMMUNICATION=False, effective_train_algo won't include agent communication
if effective_train_algo in (1,3):
    all_agent_indices = torch.stack(rollouts[35]).to(device)
    all_agent_next_indices = torch.stack(rollouts[36]).to(device)
    next_all_agent_next_indices = torch.stack(rollouts[37]).to(device)
```

**State êµ¬ì„±** (Line 264-285):
```python
# Construct state based on effective_train_algo (respects USE_COMMUNICATION setting)
if effective_train_algo == 0:
    # SAC: observation only, no communication
    state = observation
    next_state = next_observation
elif effective_train_algo == 1:
    # MAAC with communication: observation + agent indices
    state = [*observation, all_agent_indices, all_agent_next_indices]
    next_state = [*next_observation, all_agent_next_indices, next_all_agent_next_indices]
elif effective_train_algo == 2:
    # Ground truth only, no communication
    state = [gt_node_inputs, gt_node_padding_mask, ...]
    next_state = [gt_next_node_inputs, ...]
elif effective_train_algo == 3:
    # MAAC with ground truth and communication
    state = [gt_node_inputs, ..., all_agent_indices, all_agent_next_indices]
    next_state = [gt_next_node_inputs, ..., all_agent_next_indices, next_all_agent_next_indices]
```

### TRAIN_ALGOì™€ USE_COMMUNICATION ì¡°í•©

| TRAIN_ALGO | USE_COMMUNICATION | effective_train_algo | ì„¤ëª… |
|------------|-------------------|---------------------|------|
| 0 (SAC) | True | 0 | SAC, no communication |
| 0 (SAC) | False | 0 | SAC, no communication |
| 1 (MAAC) | True | 1 | MAAC with communication |
| 1 (MAAC) | False | 0 | SAC, FOV trajectory only |
| 2 (GT) | True | 2 | Ground Truth, no communication |
| 2 (GT) | False | 2 | Ground Truth, no communication |
| 3 (MAAC+GT) | True | 3 | MAAC+GT with communication |
| 3 (MAAC+GT) | False | 2 | GT only, FOV trajectory only |

### ì‚¬ìš© ì˜ˆì‹œ

#### í†µì‹  ì—†ëŠ” í˜„ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ í•™ìŠµ:
```python
# parameter.py
N_AGENTS = 4
USE_COMMUNICATION = False
TRAIN_ALGO = 3  # Will use GT only (effective_train_algo=2)
USE_CONTINUOUS_SIM = True

# Trajectory settings
TRAJECTORY_HISTORY_LENGTH = 10
TRAJECTORY_EMBEDDING_DIM = 64
MAX_DETECTED_AGENTS = 3  # N_AGENTS - 1
```

```bash
conda activate marvel
python driver.py
```

ì¶œë ¥ ì˜ˆì‹œ:
```
Training Configuration:
  TRAIN_ALGO: 3
  USE_COMMUNICATION: False
  Effective TRAIN_ALGO for QNet: 2
  Using Trajectory Encoder: True
```

#### í†µì‹  ìˆëŠ” Centralized í•™ìŠµ:
```python
# parameter.py
N_AGENTS = 4
USE_COMMUNICATION = True
TRAIN_ALGO = 3  # Will use MAAC+GT (effective_train_algo=3)
```

```bash
python driver.py
```

ì¶œë ¥ ì˜ˆì‹œ:
```
Training Configuration:
  TRAIN_ALGO: 3
  USE_COMMUNICATION: True
  Effective TRAIN_ALGO for QNet: 3
  Using Trajectory Encoder: True
```

### ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜

ë‘ ëª¨ë“œì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ë ¤ë©´:

1. **í†µì‹  ìˆëŠ” ëª¨ë¸ í•™ìŠµ**:
   ```python
   # parameter.py
   FOLDER_NAME = 'with_communication'
   USE_COMMUNICATION = True
   TRAIN_ALGO = 3
   ```

2. **í†µì‹  ì—†ëŠ” ëª¨ë¸ í•™ìŠµ**:
   ```python
   # parameter.py
   FOLDER_NAME = 'no_communication'
   USE_COMMUNICATION = False
   TRAIN_ALGO = 3
   ```

3. **TensorBoardë¡œ ë¹„êµ**:
   ```bash
   tensorboard --logdir train/
   ```

### ì£¼ì˜ì‚¬í•­

1. **ì²´í¬í¬ì¸íŠ¸ í˜¸í™˜ì„±**:
   - `USE_COMMUNICATION`ì´ ë‹¤ë¥¸ ì„¤ì •ìœ¼ë¡œ í•™ìŠµëœ ëª¨ë¸ì€ ë‹¤ë¥¸ QNet êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤
   - ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹œ ë™ì¼í•œ `USE_COMMUNICATION` ì„¤ì • í•„ìš”

2. **Episode Buffer**:
   - `USE_COMMUNICATION=False`ì¼ ë•ŒëŠ” agent indicesê°€ episode bufferì— ì €ì¥ë˜ì§€ ì•ŠìŒ
   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ íš¨ê³¼

3. **Trajectory Encoderì˜ ì—­í• **:
   - `USE_COMMUNICATION=False`ì¼ ë•Œ trajectory encoderê°€ ë”ìš± ì¤‘ìš”
   - FOV ê°ì§€ëœ ë¡œë´‡ì˜ ì •ë³´ê°€ ìœ ì¼í•œ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ ì •ë³´ì›

### Test Modeì—ì„œì˜ í†µì‹  ì„¤ì •

**test_parameter.py** (Line 77-79):
```python
# Communication settings (same as parameter.py)
USE_COMMUNICATION = False  # True: Use all agent communication
                           # False: Decentralized testing with only FOV-based trajectory observation
```

**ì¤‘ìš”**: Test modeì—ì„œëŠ” PolicyNetë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ agent ê°„ í†µì‹  ì •ë³´ê°€ ìë™ìœ¼ë¡œ ì œì™¸ë©ë‹ˆë‹¤:
- **Training**: PolicyNet (actor) + QNet (critic)
  - QNetì´ agent indicesë¥¼ ì‚¬ìš© (USE_COMMUNICATION=Trueì¼ ë•Œë§Œ)
- **Testing**: PolicyNetë§Œ ì‚¬ìš©
  - PolicyNetì€ observationë§Œ ì‚¬ìš© (agent indices ë¶ˆí•„ìš”)
  - ë”°ë¼ì„œ testì—ì„œëŠ” í•­ìƒ í†µì‹  ì—†ì´ ë™ì‘

**ì‹¤ì œ íš¨ê³¼**:
- Testì—ì„œëŠ” ê° agentê°€ ìì‹ ì˜ observation + FOV ë‚´ ê°ì§€ëœ trajectoryë§Œìœ¼ë¡œ í–‰ë™ ê²°ì •
- ë‹¤ë¥¸ agentì˜ ì „ì—­ ìœ„ì¹˜ ì •ë³´ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
- Trainingì—ì„œ `USE_COMMUNICATION=False`ë¡œ í•™ìŠµëœ ëª¨ë¸ì´ testì—ì„œ ì˜¬ë°”ë¥´ê²Œ í‰ê°€ë¨

---

## ğŸ’¡ í–¥í›„ ê°œì„  ë°©í–¥

1. **Trajectory ì˜ˆì¸¡**: ë¯¸ë˜ trajectory ì˜ˆì¸¡ ê¸°ëŠ¥ ì¶”ê°€
2. **Partial Communication**: ì œí•œì  í†µì‹  ì‹œë‚˜ë¦¬ì˜¤ (ê±°ë¦¬ ê¸°ë°˜, ëŒ€ì—­í­ ì œí•œ)
3. **Hierarchical Attention**: ì‹œê°„/ê³µê°„ ê³„ì¸µì  attention
4. **Memory Module**: Long-term trajectory memory
5. **Adaptive History Length**: ë™ì  history ê¸¸ì´ ì¡°ì •

---

**ì‘ì„±ì¼**: 2025-11-26
**ë²„ì „**: 1.2
**ì‘ì„±ì**: Claude (Anthropic)
**ìµœê·¼ ì—…ë°ì´íŠ¸**: ê° Agentë³„ local view ì‹œê°í™” ì¶”ê°€ (test_worker.py)
